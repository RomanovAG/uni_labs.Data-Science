{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c62cc95-69e4-4820-9a0c-e605ad87b25e",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0863b8-d8e1-4ae8-88d8-154e69e14de1",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Enhancing Data Science Outcomes With Efficient Workflow #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157e3b9e-1612-460a-a3b1-1560f0651441",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 03 - Model Deployment for Inference ##\n",
    "In this lab, you will learn how to deploy a model on Triton Inference Server. We'll learn how to create the model directory structures and configuration files within Triton Inference Server and how to send inference requests to the models deployed within it.\n",
    "\n",
    "**Table of Contents**\n",
    "<br>\n",
    "In this notebook, we will deploy models using Triton Inference Server with the Forest Inference Library backend. This notebook covers the below sections: \n",
    "1. [Deploying Models in Triton](#s1-1)\n",
    "    * [Introduction to Triton Inference Server](#s1-1.1)\n",
    "    * [Server](#s1-1.2)\n",
    "    * [Client](#s1-1.3)\n",
    "    * [Model Repository](#s1-1.4)\n",
    "2. [Load Existing XGBoost Model](#s1-2)\n",
    "3. [Configure Triton Inference Server](#s1-3)\n",
    "    * [Exercise #1 - Model Configuration](#s1-e1)\n",
    "4. [Run Inference on Triton Inference Server](#s1-4)\n",
    "    * [Server Health Status](#s1-4.1)\n",
    "    * [Send Inference Request to Server](#s1-4.2)\n",
    "    * [Run Batch Inference](#s1-4.3)\n",
    "5. [Conclusion](#s1-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4531dd37-e33c-4d3d-a2a4-6ddc397021bb",
   "metadata": {},
   "source": [
    "<a name='s1-1'></a>\n",
    "## Deploying Models in Triton ##\n",
    "Organizations tree-based models for a significant amount of mission-critical data, therefore it has become increasingly important to make deploying such models easy, efficient, and performant. NVIDIA [Triton Inference Server](https://developer.nvidia.com/nvidia-triton-inference-server) offers a complete solution for deploying tree models through the [Forest Inference Library backend](https://github.com/triton-inference-server/fil_backend). Based on the RAPIDS [Forest Inference Library (FIL)](https://docs.rapids.ai/api/cuml/stable/api/#forest-inferencing), the Triton Inference Server FIL backend allows us to achieve optimal throughput/latency. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1945095-75e2-4b72-b47c-d30ba6003d67",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a name='s1-1.1'></a>\n",
    "### Introduction to Triton Inference Server ###\n",
    "NVIDIA [Triton Inference Server](https://developer.nvidia.com/nvidia-triton-inference-server) simplifies the deployment of AI models at scale in production. Triton is an open-source, inference-serving software that lets teams deploy trained AI models on any GPU or CPU-based infrastructure, cloud, data center, or edge. \n",
    "\n",
    "The below figure shows the Triton Inference Server high-level architecture. The model repository is a _file-system based repository_ of the models that Triton will make available for inferencing. Inference requests arrive at the server via either [HTTP/REST](https://en.wikipedia.org/wiki/Representational_state_transfer), [gRPC](https://en.wikipedia.org/wiki/GRPC), or by the C API and are then routed to the appropriate per-model scheduler. Triton implements multiple scheduling and batching algorithms that can be configured on a model-by-model basis. Each model's scheduler optionally performs batching of inference requests and then passes the requests to the backend corresponding to the model type. The backend performs inferencing using the inputs provided in the batched requests to produce the requested outputs. The outputs are then returned.\n",
    "\n",
    "<p><img src='images/triton_server_architecture.png' width='720'/></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6b5277-b33b-4a1e-96bb-1b5570b60647",
   "metadata": {},
   "source": [
    "<a name='s1-1.2'></a>\n",
    "### Server ###\n",
    "Setting up the Triton Inference Server requires software for the server and the client. We can get started with Triton Inference Server by pulling the [container](https://ngc.nvidia.com/catalog/containers/nvidia:tritonserver) from the NVIDIA NGC catalog. In this lab, we already have Triton Inference Server instance running. The code to run a Triton Server Instance is shown below. More details can be found in the [QuickStart Documentation](https://github.com/triton-inference-server/server/blob/r20.12/docs/quickstart.md) and [Build Documentation](https://github.com/triton-inference-server/server/blob/r20.12/docs/build.md). \n",
    "\n",
    "```\n",
    "docker run \\\n",
    "  --gpus=1 \\\n",
    "  --ipc=host --rm \\\n",
    "  --shm-size=1g \\\n",
    "  --ulimit memlock=-1 \\\n",
    "  --ulimit stack=67108864 \\\n",
    "  -p 8000:8000 -p 8001:8001 -p 8002:8002 \\\n",
    "  -v /models:/models \\\n",
    "  nvcr.io/nvidia/tritonserver:20.12-py3 \\\n",
    "  tritonserver \\\n",
    "  --model-repository=/models \\\n",
    "  --exit-on-error=false \\\n",
    "  --model-control-mode=poll \\\n",
    "  --repository-poll-secs 30\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5b5615-d578-4fb8-8f61-d4eca4afa891",
   "metadata": {},
   "source": [
    "<a name='s1-1.3'></a>\n",
    "### Client ###\n",
    "We've also installed the Triton Inference Server Client libraries to provide APIs that make it easy to communicate with Triton from any C++ or Python application. Using these libraries, we can send either HTTP/REST or gRPC requests to Triton to access all its capabilities: inferencing, status and health, statistics and metrics, model repository management, etc. These libraries also support using system and CUDA shared memory for passing inputs to and receiving outputs from Triton. \n",
    "\n",
    "The easiest way to get the Python client library is to use `pip` to install the `tritonclient` module, as detailed below. For more details on how to download or build the Triton Inference Server Client libraries, please see the documentation [here](https://github.com/triton-inference-server/server/blob/r20.12/docs/client_libraries.md), as well as examples that show the use of both the C++ and Python libraries.\n",
    "\n",
    "```\n",
    "pip install nvidia-pyindex\n",
    "pip install tritonclient[all]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2a84b9-2dd7-4ddb-8333-3add28b986ed",
   "metadata": {},
   "source": [
    "<a name='s1-1.4'></a>\n",
    "### Model Repository ###\n",
    "Triton Inference Server serves models within a model repository. When we started Triton Inference Server, we needed to specify the model repository where the models reside:\n",
    "\n",
    "```\n",
    "tritonserver --model-repository=/models\n",
    "```\n",
    "\n",
    "Each model resides in its own model subdirectory within the model repository - i.e., each directory within `/models` represents a unique model. For example, in this notebook we'll be deploying our `classification_model`. All models typically follow a similar directory structure. Within each of these directories, we'll create a configuration file `config.pbtxt` that details information about the model - e.g., _batch size_, _input shapes_, _deployment backend_ (FIL, PyTorch, ONNX, TensorFlow, TensorRT, etc.) and more. Additionally, we can create one or more versions of our model. Each version lives under a subdirectory name with the respective version number, starting with `1`. It's within this subdirectory where our model files reside. \n",
    "\n",
    "```\n",
    "root@server:/models$ tree\n",
    ".\n",
    "├── classification_model\n",
    "│   ├── 1\n",
    "│   │   └── xgboost.json\n",
    "│   └── config.pbtxt\n",
    "│\n",
    "\n",
    "```\n",
    "\n",
    "For more details on how to work with model repositories and model directory structures in Triton Inference Server, please see the [documentation](https://github.com/triton-inference-server/server/blob/r20.12/docs/model_repository.md). Below, we'll create the model directory structure for our classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67b24e39-7831-4a90-87ca-f85d7c41daeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p models/classification_model/1\n",
    "\n",
    "!cp sample_xgboost.json models/classification_model/1/xgboost.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d890d568-f7dd-4b77-bfec-db3b97c1840a",
   "metadata": {},
   "source": [
    "<a name='s1-2'></a>\n",
    "## Load Existing XGBoost Model ##\n",
    "We will compare model inference performance between XGBoost and Triton Inference Server. We start by loading an existing XGBoost model that is similar to the ones we created from the previous exercises. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b735d81d-2bee-4c3d-927f-86bdeadf147f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-24 21:22:27,709 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize\n",
      "2023-12-24 21:22:27,709 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize\n",
      "2023-12-24 21:22:27,709 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize\n",
      "2023-12-24 21:22:27,709 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize\n",
      "2023-12-24 21:22:27,723 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize\n",
      "2023-12-24 21:22:27,723 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize\n",
      "2023-12-24 21:22:27,728 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize\n",
      "2023-12-24 21:22:27,728 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize\n"
     ]
    }
   ],
   "source": [
    "# import dependencies\n",
    "from dask.distributed import Client, wait\n",
    "from dask_cuda import LocalCUDACluster\n",
    "from cuml.metrics import accuracy_score\n",
    "\n",
    "import dask_cudf\n",
    "import xgboost\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# instantiate a Client\n",
    "cluster=LocalCUDACluster()\n",
    "client=Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82e91544-7668-474f-af40-d68ade054ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing list\n",
    "features_list=['brand', 'cat_0', 'cat_1', 'cat_2', 'cat_3', \n",
    "               'price', 'ts_hour', 'ts_minute', 'ts_weekday', \n",
    "               'brand_target_sum', 'brand_count', 'cat_0_target_sum', \n",
    "               'cat_0_count', 'cat_1_target_sum', 'cat_1_count', \n",
    "               'cat_2_target_sum', 'cat_2_count', 'cat_3_target_sum', \n",
    "               'cat_3_count', 'TE_brand_target', 'TE_cat_0_target', \n",
    "               'TE_cat_1_target', 'TE_cat_2_target', 'TE_cat_3_target', \n",
    "               'relative_price_product', 'relative_price_category']\n",
    "         \n",
    "include=features_list+['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1465472-3060-4ef3-b0b4-060f877b6757",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total of 2461697 records. \n"
     ]
    }
   ],
   "source": [
    "# load data with dask_cudf\n",
    "parquet_dir='processed_parquet'\n",
    "ddf=dask_cudf.read_parquet(parquet_dir, columns=include)\n",
    "\n",
    "# load data into single GPU memory\n",
    "gdf=ddf.compute()\n",
    "\n",
    "print(f'Total of {len(gdf)} records. ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2dab00c1-c50d-4444-adf0-011bebbb2253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "xgb_clf=xgboost.Booster()\n",
    "xgb_clf.load_model('sample_xgboost.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55943dbf-6527-41c4-a14c-7a2b24ce64a3",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 62.37%\n",
      "Throughput is: 2506723.92 per seconds. \n"
     ]
    }
   ],
   "source": [
    "# XGBoost treats all data as 32-bit float internally\n",
    "y=gdf['target'].astype('float32')\n",
    "X=gdf[features_list].astype('float32')\n",
    "\n",
    "# create DMatrix\n",
    "dmatrix=xgboost.DMatrix(X, y) \n",
    "\n",
    "# predict\n",
    "start=time.time()\n",
    "prediction_test=xgb_clf.predict(dmatrix)\n",
    "elapsed_time=time.time()-start\n",
    "\n",
    "print(f'Accuracy score: {round(float(accuracy_score(y, prediction_test>0.5))*100, 2)}%')\n",
    "\n",
    "xgboost_throughput=round(len(X)/elapsed_time, 2)\n",
    "print(f'Throughput is: {xgboost_throughput} per seconds. ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f46772-44aa-4ae3-bb98-c7d59c639f14",
   "metadata": {},
   "source": [
    "<a name='s1-3'></a>\n",
    "## Configure Triton Inference Server ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42112f92-cf4b-484a-8196-fc00ecedf248",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a name='s1-e1'></a>\n",
    "### Exercise #1 - Model Configuration ###\n",
    "With our model directory set up, we now turn our attention to creating the configuration file for our model. A minimal model configuration must specify the name of the model, the `platform` and/or backend properties, the `max_batch_size` property, and the `input` and `output` tensors of the model (name, data type, and shape). For more details on how to create model configuration files within Triton Inference Server, please see the [documentation](https://github.com/triton-inference-server/server/blob/r20.12/docs/model_configuration.md). \n",
    "* `max_batch_size`: the maximum batch size that can be passed to this model. In general, the only limit on the size of batches passed to a FIL backend is the memory available with which to process them. For GPU execution, the available memory is determined by the size of Triton's CUDA memory pool, which can be set via a command line argument when starting the server\n",
    "* `input`: options in this section tell Triton the number of features to expect for each input sample\n",
    "* `output`: options in this section tell Triton how many output values there will be for each sample. If the \"predict_proba\" option (described further on) is set to true, then a probability value will be returned for each class. Otherwise, a single value will be returned indicating the class predicted for the given sample\n",
    "* `instance_group`: this determines how many instances of this model will be created and whether they will use the GPU or CPU.\n",
    "* `model_type`: a string indicating what format the model is in (\"xgboost_json\" in this example, but \"xgboost\", \"lightgbm\", and \"tl_checkpoint\" are valid formats as well)\n",
    "* `predict_proba`: if set to `True`, probability values will be returned for each class rather than just a class prediction\n",
    "* `output_class`: `True` for classification models, `False` for regression models\n",
    "* `threshold`: a score threshold for determining classification. When `output_class` is set to `True`, this must be provided, although it will not be used if predict_proba is also set to `True`\n",
    "* `storage_type`: in general, using \"AUTO\" for this setting should meet most usecases. If \"AUTO\" storage is selected, FIL will load the model using either a sparse or dense representation based on the approximate size of the model. In some cases, you may want to explicitly set this to \"SPARSE\" in order to reduce the memory footprint of large models.\n",
    "\n",
    "**Instructions**:<br>\n",
    "* Modify the `<FIXME>`s only and execute the cell to create the `config.pbtxt` file for the defect classification model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "beaa27f7-4f68-4b92-bbd9-8fd2ad52dca0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "features=len(features_list)\n",
    "num_classes=2\n",
    "\n",
    "bytes_per_sample = (features + num_classes) * 4\n",
    "max_batch_size = 50000 # or MAX_MEMORY_BYTES // bytes_per_sample\n",
    "\n",
    "configuration = f\"\"\"\n",
    "backend: \"fil\"\n",
    "max_batch_size: {max_batch_size}\n",
    "input: [\n",
    "  {{\n",
    "    name: \"input__0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ {features} ]\n",
    "  }}\n",
    "]\n",
    "output: [\n",
    "  {{\n",
    "    name: \"output__0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ {num_classes} ]\n",
    "  }}\n",
    "]\n",
    "instance_group [{{ kind: KIND_GPU }}]\n",
    "parameters [\n",
    "  {{\n",
    "    key: \"model_type\"\n",
    "    value: {{ string_value: \"xgboost_json\" }}\n",
    "  }},\n",
    "  {{\n",
    "    key: \"predict_proba\"\n",
    "    value: {{ string_value: \"true\" }}\n",
    "  }},\n",
    "  {{\n",
    "    key: \"output_class\"\n",
    "    value: {{ string_value: \"true\" }}\n",
    "  }},\n",
    "  {{\n",
    "    key: \"threshold\"\n",
    "    value: {{ string_value: \"0.5\" }}\n",
    "  }}\n",
    "]\n",
    "dynamic_batching {{\n",
    "  max_queue_delay_microseconds: 100\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "with open('models/classification_model/config.pbtxt', 'w') as file:\n",
    "    file.write(configuration)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "94354e2f-cd0f-4539-b891-3f7b094cb0dd",
   "metadata": {
    "tags": []
   },
   "source": [
    "features=len(features_list)\n",
    "num_classes=2\n",
    "\n",
    "bytes_per_sample = (features + num_classes) * 4\n",
    "max_batch_size = 50000 # or MAX_MEMORY_BYTES // bytes_per_sample\n",
    "\n",
    "configuration = f\"\"\"\n",
    "backend: \"fil\"\n",
    "max_batch_size: {max_batch_size}\n",
    "input: [\n",
    "  {{\n",
    "    name: \"input__0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ {features} ]\n",
    "  }}\n",
    "]\n",
    "output: [\n",
    "  {{\n",
    "    name: \"output__0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ {num_classes} ]\n",
    "  }}\n",
    "]\n",
    "instance_group [{{ kind: KIND_GPU }}]\n",
    "parameters [\n",
    "  {{\n",
    "    key: \"model_type\"\n",
    "    value: {{ string_value: \"xgboost_json\" }}\n",
    "  }},\n",
    "  {{\n",
    "    key: \"predict_proba\"\n",
    "    value: {{ string_value: \"true\" }}\n",
    "  }},\n",
    "  {{\n",
    "    key: \"output_class\"\n",
    "    value: {{ string_value: \"true\" }}\n",
    "  }},\n",
    "  {{\n",
    "    key: \"threshold\"\n",
    "    value: {{ string_value: \"0.5\" }}\n",
    "  }}\n",
    "]\n",
    "dynamic_batching {{\n",
    "  max_queue_delay_microseconds: 100\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "with open('models/classification_model/config.pbtxt', 'w') as file:\n",
    "    file.write(configuration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e90389-8969-406b-b1bc-b86111a847cc",
   "metadata": {},
   "source": [
    "Click ... to show **solution**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39ba239-eb7d-4c72-b8c7-9b70074ba545",
   "metadata": {},
   "source": [
    "<a name='s1-4'></a>\n",
    "## Run Inference on Triton Inference Server ##\n",
    "With our model directory structures created, models defined and exported, and configuration files created, we will now wait for Triton Inference Server to load our models. We have set up this lab to use Triton Inference Server in **polling** mode. This means that Triton Inference Server will continuously poll for modifications to our models or for newly created models - once every 30 seconds. Please run the cell below to allow time for Triton Inference Server to poll for new models/modifications before proceeding. Due to the asynchronous nature of this step, we have added 15 seconds to be safe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3800be-ce51-4bcc-a353-fdcf03806f21",
   "metadata": {},
   "source": [
    "<a name='s1-4.1'></a>\n",
    "### Server Health Status ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4645417-1f93-4456-b6e3-0c7c2e7b3412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "!sleep 45"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49aeb80-4b61-43c9-a194-f4dc0ad00e02",
   "metadata": {},
   "source": [
    "At this point, our models should be deployed and ready to use! To confirm Triton Inference Server is up and running, we can send a `curl` request to the below URL. The HTTP request returns status _200_ if Triton is ready and _non-200_ if it is not ready. We can also send a `curl` request to our model endpoints to confirm our models are deployed and ready to use. Additionally, we will also see information about our models such:\n",
    "* The name of our model\n",
    "* The versions available for our model\n",
    "* The backend platform (e.g., tensort_rt, pytorch_libtorch, onnxruntime_onnx)\n",
    "* The inputs and outputs, with their respective names, data types, and shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e7eb618-7d6b-48d2-8937-2a8b37ab072f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*   Trying 172.18.0.2:8000...\n",
      "* Connected to triton (172.18.0.2) port 8000 (#0)\n",
      "> GET /v2/health/ready HTTP/1.1\n",
      "> Host: triton:8000\n",
      "> User-Agent: curl/7.86.0\n",
      "> Accept: */*\n",
      "> \n",
      "* Mark bundle as not supporting multiuse\n",
      "< HTTP/1.1 200 OK\n",
      "< Content-Length: 0\n",
      "< Content-Type: text/plain\n",
      "< \n",
      "* Connection #0 to host triton left intact\n"
     ]
    }
   ],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "!curl -v triton:8000/v2/health/ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a626d1a8-ba1c-446c-a53d-891ed78d1949",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*   Trying 172.18.0.2:8000...\n",
      "* Connected to triton (172.18.0.2) port 8000 (#0)\n",
      "> GET /v2/models/classification_model HTTP/1.1\n",
      "> Host: triton:8000\n",
      "> User-Agent: curl/7.86.0\n",
      "> Accept: */*\n",
      "> \n",
      "* Mark bundle as not supporting multiuse\n",
      "< HTTP/1.1 200 OK\n",
      "< Content-Type: application/json\n",
      "< Content-Length: 196\n",
      "< \n",
      "* Connection #0 to host triton left intact\n",
      "{\"name\":\"classification_model\",\"versions\":[\"1\"],\"platform\":\"fil\",\"inputs\":[{\"name\":\"input__0\",\"datatype\":\"FP32\",\"shape\":[-1,26]}],\"outputs\":[{\"name\":\"output__0\",\"datatype\":\"FP32\",\"shape\":[-1,2]}]}"
     ]
    }
   ],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "!curl -v triton:8000/v2/models/classification_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93558b9e-43d7-4d2a-8f9f-3a87129f7ca5",
   "metadata": {},
   "source": [
    "<a name='s1-4.2'></a>\n",
    "### Send Inference Request to Server ###\n",
    "With our models deployed, it is now time to send inference requests to our models. First, we'll load the `tritonclient.http` module. We will also define the input and output names of our model, the name of our model, the URL where our models are deployed with Triton Inference Server (in this case the host `triton:8000`), and our model version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0aaa62e7-d16d-41ee-a712-5b39229a6463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependencies\n",
    "import tritonclient.http as tritonhttpclient\n",
    "from tritonclient import utils as triton_utils\n",
    "HOST='triton'\n",
    "PORT=8000\n",
    "TIMEOUT=60\n",
    "VERBOSE=False\n",
    "model_name='classification_model'\n",
    "model_version='1'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d03260-a4e7-4668-8cd8-0d88a371562b",
   "metadata": {},
   "source": [
    "We'll instantiate our client `triton_client` using the `tritonhttpclient.InferenceServerClient` class access the model metadata with the `get_model_metadata()` method as well as get our model configuration with the `get_model_config()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "83d52a64-ef23-44f5-b41f-66f90469d33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate client\n",
    "triton_client=tritonhttpclient.InferenceServerClient(url=f'{HOST}:{PORT}', verbose=VERBOSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548ca85d-f46f-46bc-b0b0-27a965ebb4c8",
   "metadata": {},
   "source": [
    "We'll instantiate a placeholder for our input data using the input name, shape, and data type expected. We'll set the data of the input to be the NumPy array representation of our image. We'll also instantiate a placeholder for our output data using just the output name. Lastly, we'll submit our input to the Triton Inference Server using the `triton_client.infer()` method, specifying our model name, model version, inputs, and outputs and convert our result to a NumPy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0a17fdb1-a630-4455-8e53-31ecff0fc120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to make inference request\n",
    "def triton_predict(model_name, arr):\n",
    "    triton_input = tritonhttpclient.InferInput('input__0', arr.shape, 'FP32')\n",
    "    triton_input.set_data_from_numpy(arr)\n",
    "    triton_output = tritonhttpclient.InferRequestedOutput('output__0')\n",
    "    response = triton_client.infer(model_name, model_version='1', inputs=[triton_input], outputs=[triton_output])\n",
    "    return response.as_numpy('output__0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4c28d48c-e5de-4039-8c52-a2b5372d9235",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result computed on Triton: \n",
      "[[0.49424398 0.505756  ]\n",
      " [0.4811334  0.5188666 ]\n",
      " [0.53086543 0.4691346 ]\n",
      " [0.58638287 0.41361716]\n",
      " [0.54397285 0.45602712]]\n"
     ]
    }
   ],
   "source": [
    "# conver to NumPy array\n",
    "np_data=X.to_numpy()\n",
    "\n",
    "triton_result=triton_predict('classification_model', np_data[0:5])\n",
    "\n",
    "print(\"Result computed on Triton: \")\n",
    "print(triton_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e23024-4378-468f-8d20-a7f6599d698c",
   "metadata": {},
   "source": [
    "<a name='s1-4.3'></a>\n",
    "### Run Batch Inference ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "829f7783-27ff-44b5-90ec-67f06493d629",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Throughput is: 1977689.88 per seconds. \n"
     ]
    }
   ],
   "source": [
    "# run batch inference\n",
    "time_list=[]\n",
    "batch_size=10000\n",
    "iterations=len(np_data)//batch_size\n",
    "count=0\n",
    "\n",
    "# iterate until no last batch\n",
    "for i in range(iterations): \n",
    "    start=time.time()\n",
    "    response=triton_predict('classification_model', np_data[count:count+batch_size])\n",
    "    time_list.append(time.time()-start)\n",
    "    count+=batch_size\n",
    "\n",
    "triton_throughput=round(count/sum(time_list), 2)\n",
    "print(f'Throughput is: {triton_throughput} per seconds. ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f50212a4-daca-41dc-82a3-faa9bcd4afc2",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot comparison\n",
    "plt.bar(x=['XGBoost', 'Triton'], height=[xgboost_throughput, triton_throughput])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd11caf3-55ff-406d-bc2d-f1dce2713240",
   "metadata": {},
   "source": [
    "<a name='s1-5'></a>\n",
    "## Conclusion ##\n",
    "In this notebook, we showed how to deploy an XGBoost model in Triton using the FIL backend. While it is possible to deploy these models on both CPU and GPU in Triton, GPU-deployed models offer far higher throughput at lower latency. As a result, we can deploy more sophisticated models on the GPU for any given latency budget and thereby obtain far more accurate results. While we have focused on XGBoost in this example, FIL also natively supports LightGBM's text serialization format as well as Treelite's checkpoint format. Thus, the same general steps can be used to serve LightGBM models and any Treelite-convertible model (including scikit-learn and cuML forest models). With the new FIL backend, Triton is now ready to serve forest models of all kinds in production, whether on their own or in concert with any of the deep-learning models supported by Triton."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3cdd1f-5a5c-4d23-a095-c8c46e08efdd",
   "metadata": {},
   "source": [
    "**Well Done!** When you're finished, please complete the assessment before moving onto the survey. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210dc84b-ccd8-4295-87cf-897090d19c42",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
